{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f68364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Starting Week 6: Data Retrieval and Processing (CORRECTED VERSION)\n",
      "🎯 Output: Clean dataset ready for Tableau dashboards and line charts\n",
      "======================================================================\n",
      "🔗 Connecting to blockchain...\n",
      "✅ Connected to Ganache successfully!\n",
      "✅ Connected to Smart Contract\n",
      "\n",
      "📁 Looking for logistics data file...\n",
      "📋 CSV files found: ['cleaned_humidity_data.csv', 'cleaned_iot_data.csv', 'cleaned_shock_data.csv', 'cleaned_temperature_data.csv', 'iot_data_summary.csv', 'Logistics-Data.csv', 'W6-Cleaned_Iot_Data.csv']\n",
      "✅ Found and loaded: Logistics-Data.csv\n",
      "📊 Data shape: (100, 10)\n",
      "✅ All required columns found!\n",
      "\n",
      "📊 Checking blockchain data...\n",
      "📦 Total packages: 100\n",
      "📈 Blockchain records found: 300\n",
      "\n",
      "📥 Retrieving 300 records from blockchain...\n",
      "🕐 Using ORIGINAL CSV timestamps (not blockchain timestamps)\n",
      "📅 Timeline will match your original logistics data\n",
      "📊 Original data timeline: 2025-04-29 12:08:30.336512 to 2025-05-06 10:09:30.336512\n",
      "🔍 Testing first device: PKG85046\n",
      "📊 Test device has 3 records\n",
      "🧪 Test record structure: ['temperature', '26.0', 1750487450]\n",
      "   Data Type: temperature\n",
      "   Value: 26.0\n",
      "   Blockchain Timestamp: 1750487450 (will ignore this)\n",
      "   CSV Timestamp: 2025-04-29 12:08:30.336512 (will use this)\n",
      "🔍 Record 0: ['temperature', '26.0', 1750487450]\n",
      "   Using CSV timestamp: 2025-04-29 12:08:30.336512\n",
      "🧪 First record structure: {'timestamp': Timestamp('2025-04-29 12:08:30.336512'), 'device_id': 'PKG85046', 'data_type': 'temperature', 'data_value': '26.0', 'location': '33.3012, 13.5857', 'closest_city': 'Cairo', 'status': 'In Transit', 'origin': 'São Paulo', 'destination': 'Moscow'}\n",
      "🔍 Record 1: ['humidity', '51.6', 1750487450]\n",
      "   Using CSV timestamp: 2025-04-29 12:08:30.336512\n",
      "🔍 Record 2: ['shock', '0.2', 1750487450]\n",
      "   Using CSV timestamp: 2025-04-29 12:08:30.336512\n",
      "📥 Retrieved 100/300 records...\n",
      "📥 Retrieved 200/300 records...\n",
      "📥 Retrieved 300/300 records...\n",
      "✅ Retrieved 300 records from blockchain!\n",
      "📊 Data list length: 300\n",
      "🔍 Sample data item keys: ['timestamp', 'device_id', 'data_type', 'data_value', 'location', 'closest_city', 'status', 'origin', 'destination']\n",
      "🕐 Using original CSV timeline: 2025-04-29 12:08:30.336512\n",
      "\n",
      "🔧 Processing data for Tableau and line charts...\n",
      "📊 DataFrame shape: (300, 9)\n",
      "📋 Columns found: ['timestamp', 'device_id', 'data_type', 'data_value', 'location', 'closest_city', 'status', 'origin', 'destination']\n",
      "📝 First record sample:\n",
      "[{'timestamp': Timestamp('2025-04-29 12:08:30.336512'), 'device_id': 'PKG85046', 'data_type': 'temperature', 'data_value': '26.0', 'location': '33.3012, 13.5857', 'closest_city': 'Cairo', 'status': 'In Transit', 'origin': 'São Paulo', 'destination': 'Moscow'}]\n",
      "🕐 Converting timestamps...\n",
      "✅ Timestamps converted successfully\n",
      "🔢 Extracting numeric values...\n",
      "🛡️ Enforcing positive values only (IoT sensors don't produce negative readings)\n",
      "🧹 Cleaning and validating data...\n",
      "🔍 Applying sensor-specific validation...\n",
      "✅ All numeric values are positive. Minimum value: 0.01\n",
      "\n",
      "📊 Data Quality Report:\n",
      "   ✅ Total records: 300\n",
      "   📦 Unique packages: 100\n",
      "   📈 Sensor types: humidity, shock, temperature\n",
      "   🌍 Cities: 19\n",
      "   📅 Date range: 2025-04-29 12:08:30.336512 to 2025-05-06 10:09:30.336512\n",
      "\n",
      "🔍 Sensor Data Validation (ALL VALUES POSITIVE):\n",
      "   temperature: 100 readings\n",
      "     Range: 0.60 - 42.50\n",
      "     Average: 19.40 ± 9.93\n",
      "     ✅ All temperature values are positive\n",
      "   humidity: 100 readings\n",
      "     Range: 41.70 - 96.00\n",
      "     Average: 74.05 ± 11.38\n",
      "     ✅ All humidity values are positive\n",
      "   shock: 100 readings\n",
      "     Range: 0.01 - 1.98\n",
      "     Average: 0.30 ± 0.32\n",
      "     ✅ All shock values are positive\n",
      "\n",
      "📊 Records per sensor type:\n",
      "   humidity: 100 records (33.3%)\n",
      "   shock: 100 records (33.3%)\n",
      "   temperature: 100 records (33.3%)\n",
      "\n",
      "⏰ Timeline validation (using ORIGINAL CSV timeline):\n",
      "   Time span: 6 days 22:01:00\n",
      "   Records per day: 50.0\n",
      "   📅 Data covers: 2025-04-29 12:08:30.336512 to 2025-05-06 10:09:30.336512\n",
      "\n",
      "💾 Saving dataset optimized for Tableau and line charts...\n",
      "✅ Main dataset saved: cleaned_iot_data.csv\n",
      "✅ Temperature dataset saved: cleaned_temperature_data.csv\n",
      "✅ Humidity dataset saved: cleaned_humidity_data.csv\n",
      "✅ Shock dataset saved: cleaned_shock_data.csv\n",
      "✅ Summary statistics saved: iot_data_summary.csv\n",
      "\n",
      "📋 TABLEAU PREPARATION COMPLETE!\n",
      "======================================================================\n",
      "📁 FILES CREATED:\n",
      "   cleaned_iot_data.csv - Main dataset for Tableau dashboard\n",
      "   cleaned_temperature_data.csv - Temperature sensor only\n",
      "   cleaned_humidity_data.csv - Humidity sensor only\n",
      "   cleaned_shock_data.csv - Shock sensor only\n",
      "   iot_data_summary.csv - Statistical summary\n",
      "\n",
      "🎉 SUCCESS! Dataset ready for visualization!\n",
      "📊 Total: 300 records from 100 packages\n",
      "🕐 Original timeline preserved: 2025-04-29 12:08:30.336512 to 2025-05-06 10:09:30.336512\n",
      "✅ ALL VALUES POSITIVE - No negative readings in dataset\n",
      "🚀 Ready for Tableau dashboard and line chart creation!\n"
     ]
    }
   ],
   "source": [
    "from web3 import Web3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ============================================================================= \n",
    "# WEEK 6: CORRECTED DATA RETRIEVAL AND PROCESSING FOR TABLEAU/CHARTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"📥 Starting Week 6: Data Retrieval and Processing (CORRECTED VERSION)\")\n",
    "print(\"🎯 Output: Clean dataset ready for Tableau dashboards and line charts\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Connect to Ganache blockchain\n",
    "print(\"🔗 Connecting to blockchain...\")\n",
    "\n",
    "ganache_url = \"http://127.0.0.1:7545\"\n",
    "web3 = Web3(Web3.HTTPProvider(ganache_url))\n",
    "\n",
    "if web3.is_connected():\n",
    "    print(\"✅ Connected to Ganache successfully!\")\n",
    "else:\n",
    "    print(\"❌ Connection failed. Make sure Ganache is running.\")\n",
    "    exit()\n",
    "\n",
    "# Contract details\n",
    "contract_address = web3.to_checksum_address(\"0x327ef98ba37138026c79783e02d2a752ecd58f9e\")\n",
    "contract_abi = [\n",
    "    {\n",
    "        \"inputs\": [{\"internalType\": \"address\",\"name\": \"device\",\"type\": \"address\"}],\n",
    "        \"name\": \"authorizeDevice\",\"outputs\": [],\"stateMutability\": \"nonpayable\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [{\"internalType\": \"address\",\"name\": \"device\",\"type\": \"address\"}],\n",
    "        \"name\": \"revokeDevice\",\"outputs\": [],\"stateMutability\": \"nonpayable\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            {\"internalType\": \"string\",\"name\": \"deviceID\",\"type\": \"string\"},\n",
    "            {\"internalType\": \"string\",\"name\": \"dataType\",\"type\": \"string\"},\n",
    "            {\"internalType\": \"string\",\"name\": \"value\",\"type\": \"string\"}\n",
    "        ],\n",
    "        \"name\": \"storeData\",\"outputs\": [],\"stateMutability\": \"nonpayable\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [],\"stateMutability\": \"nonpayable\",\"type\": \"constructor\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [{\"internalType\": \"address\",\"name\": \"\",\"type\": \"address\"}],\n",
    "        \"name\": \"authorizedDevices\",\"outputs\": [{\"internalType\": \"bool\",\"name\": \"\",\"type\": \"bool\"}],\n",
    "        \"stateMutability\": \"view\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            {\"internalType\": \"string\",\"name\": \"deviceID\",\"type\": \"string\"},\n",
    "            {\"internalType\": \"uint256\",\"name\": \"index\",\"type\": \"uint256\"}\n",
    "        ],\n",
    "        \"name\": \"getDataByIndex\",\n",
    "        \"outputs\": [\n",
    "            {\"internalType\": \"string\",\"name\": \"\",\"type\": \"string\"},\n",
    "            {\"internalType\": \"string\",\"name\": \"\",\"type\": \"string\"},\n",
    "            {\"internalType\": \"uint256\",\"name\": \"\",\"type\": \"uint256\"}\n",
    "        ],\n",
    "        \"stateMutability\": \"view\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [{\"internalType\": \"string\",\"name\": \"deviceID\",\"type\": \"string\"}],\n",
    "        \"name\": \"getDataCount\",\n",
    "        \"outputs\": [{\"internalType\": \"uint256\",\"name\": \"\",\"type\": \"uint256\"}],\n",
    "        \"stateMutability\": \"view\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [],\"name\": \"owner\",\n",
    "        \"outputs\": [{\"internalType\": \"address\",\"name\": \"\",\"type\": \"address\"}],\n",
    "        \"stateMutability\": \"view\",\"type\": \"function\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Load the smart contract\n",
    "contract = web3.eth.contract(address=contract_address, abi=contract_abi)\n",
    "web3.eth.default_account = web3.eth.accounts[0]\n",
    "print(\"✅ Connected to Smart Contract\")\n",
    "\n",
    "# Step 2: Find and load CSV file for reference data\n",
    "print(\"\\n📁 Looking for logistics data file...\")\n",
    "\n",
    "possible_files = [\n",
    "    \"LogisticsData.csv\",\n",
    "    \"Logistics-Data.csv\", \n",
    "    \"logistics_data.csv\",\n",
    "    \"logistics-data.csv\",\n",
    "    \"IoT_Data.csv\",\n",
    "    \"iot_data.csv\"\n",
    "]\n",
    "\n",
    "df_original = None\n",
    "csv_filename = None\n",
    "\n",
    "# Check current directory for CSV files\n",
    "current_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "print(f\"📋 CSV files found: {current_files}\")\n",
    "\n",
    "# Try to find the logistics data file\n",
    "for filename in possible_files:\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            df_original = pd.read_csv(filename)\n",
    "            csv_filename = filename\n",
    "            print(f\"✅ Found and loaded: {filename}\")\n",
    "            print(f\"📊 Data shape: {df_original.shape}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {filename}: {e}\")\n",
    "\n",
    "# If still no file found, try the first CSV file in directory\n",
    "if df_original is None and current_files:\n",
    "    try:\n",
    "        csv_filename = current_files[0]\n",
    "        df_original = pd.read_csv(csv_filename)\n",
    "        print(f\"✅ Using first available CSV: {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {csv_filename}: {e}\")\n",
    "\n",
    "if df_original is None:\n",
    "    print(\"❌ No logistics data file found. Please ensure you have the logistics CSV file.\")\n",
    "    exit()\n",
    "\n",
    "# Verify required columns exist\n",
    "required_columns = ['package_id', 'temperature', 'humidity', 'shock']\n",
    "missing_columns = [col for col in required_columns if col not in df_original.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"❌ Missing required columns: {missing_columns}\")\n",
    "    print(\"📋 Available columns:\", list(df_original.columns))\n",
    "    exit()\n",
    "\n",
    "print(\"✅ All required columns found!\")\n",
    "\n",
    "# Step 3: Get blockchain data count\n",
    "print(\"\\n📊 Checking blockchain data...\")\n",
    "\n",
    "device_ids = df_original[\"package_id\"].unique()\n",
    "total_records = 0\n",
    "\n",
    "for device_id in device_ids:\n",
    "    try:\n",
    "        device_count = contract.functions.getDataCount(str(device_id)).call()\n",
    "        total_records += device_count\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error checking records for {device_id}: {e}\")\n",
    "\n",
    "print(f\"📦 Total packages: {len(device_ids)}\")\n",
    "print(f\"📈 Blockchain records found: {total_records}\")\n",
    "\n",
    "if total_records == 0:\n",
    "    print(\"⚠️ No data on blockchain!\")\n",
    "    print(\"🚀 Will automatically store CSV data to blockchain first...\")\n",
    "    \n",
    "    # STORE ALL DATA TO BLOCKCHAIN\n",
    "    print(f\"\\n📤 Storing IoT data to blockchain...\")\n",
    "    \n",
    "    # Authorize the current account first\n",
    "    try:\n",
    "        print(\"🔑 Authorizing device...\")\n",
    "        auth_txn = contract.functions.authorizeDevice(web3.eth.default_account).transact({\n",
    "            'from': web3.eth.default_account,\n",
    "            'gas': 3000000\n",
    "        })\n",
    "        web3.eth.wait_for_transaction_receipt(auth_txn)\n",
    "        print(\"✅ Device authorized successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Authorization issue (may already exist): {e}\")\n",
    "    \n",
    "    # Store ALL data from CSV to blockchain\n",
    "    stored_count = 0\n",
    "    failed_count = 0\n",
    "    expected_records = len(df_original) * 3  # 3 sensors per package\n",
    "    \n",
    "    print(f\"📦 Storing data from {len(df_original)} packages...\")\n",
    "    print(f\"🎯 Will create {expected_records} blockchain records...\")\n",
    "    \n",
    "    for index, row in df_original.iterrows():\n",
    "        device_id = str(row[\"package_id\"])\n",
    "        \n",
    "        # Store temperature, humidity, and shock data with validation\n",
    "        sensors = [\n",
    "            (\"temperature\", abs(float(row[\"temperature\"]))),  # Ensure positive\n",
    "            (\"humidity\", abs(float(row[\"humidity\"]))),        # Ensure positive\n",
    "            (\"shock\", abs(float(row[\"shock\"])))               # Ensure positive\n",
    "        ]\n",
    "        \n",
    "        # Show progress every 10 packages\n",
    "        if index % 10 == 0:\n",
    "            print(f\"📊 Progress: {index+1}/{len(df_original)} packages ({stored_count} records stored)...\")\n",
    "        \n",
    "        for sensor_type, sensor_value in sensors:\n",
    "            # Additional validation: Ensure reasonable ranges\n",
    "            if sensor_type == \"temperature\" and sensor_value > 60:\n",
    "                sensor_value = 60  # Cap extreme temperatures\n",
    "            elif sensor_type == \"humidity\" and sensor_value > 100:\n",
    "                sensor_value = 100  # Cap humidity at 100%\n",
    "            elif sensor_type == \"shock\" and sensor_value > 10:\n",
    "                sensor_value = 10  # Cap extreme shock values\n",
    "            \n",
    "            try:\n",
    "                txn = contract.functions.storeData(device_id, sensor_type, str(sensor_value)).transact({\n",
    "                    'from': web3.eth.default_account,\n",
    "                    'gas': 3000000\n",
    "                })\n",
    "                web3.eth.wait_for_transaction_receipt(txn)\n",
    "                stored_count += 1\n",
    "                \n",
    "                # Show detailed progress for first few records\n",
    "                if stored_count <= 5:\n",
    "                    print(f\"   🛰️ Stored [{device_id}] - {sensor_type}: {sensor_value}\")\n",
    "                \n",
    "                # Small delay to prevent overwhelming Ganache\n",
    "                time.sleep(0.05)\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                print(f\"   ❌ Failed to store {sensor_type} for {device_id}: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Data storage completed!\")\n",
    "    print(f\"✅ Successfully stored: {stored_count} records\")\n",
    "    print(f\"❌ Failed to store: {failed_count} records\")\n",
    "    if stored_count + failed_count > 0:\n",
    "        print(f\"📊 Success rate: {(stored_count/(stored_count+failed_count)*100):.1f}%\")\n",
    "    \n",
    "    # Update total_records for retrieval\n",
    "    total_records = stored_count\n",
    "    print(f\"📈 Blockchain now contains {total_records} records\")\n",
    "    \n",
    "    if total_records == 0:\n",
    "        print(\"❌ No data was stored successfully. Check blockchain connection.\")\n",
    "        exit()\n",
    "\n",
    "# NOW RETRIEVE THE DATA FROM BLOCKCHAIN\n",
    "print(f\"\\n📥 Retrieving {total_records} records from blockchain...\")\n",
    "print(\"🕐 Using ORIGINAL CSV timestamps (not blockchain timestamps)\")\n",
    "print(\"📅 Timeline will match your original logistics data\")\n",
    "\n",
    "data = []\n",
    "retrieved_count = 0\n",
    "timestamp_errors = 0\n",
    "\n",
    "# Show expected timestamp range from CSV\n",
    "csv_timestamps = pd.to_datetime(df_original[\"timestamp\"])\n",
    "print(f\"📊 Original data timeline: {csv_timestamps.min()} to {csv_timestamps.max()}\")\n",
    "\n",
    "# Test first device to debug\n",
    "if len(device_ids) > 0:\n",
    "    test_device = device_ids[0]\n",
    "    print(f\"🔍 Testing first device: {test_device}\")\n",
    "    try:\n",
    "        test_count = contract.functions.getDataCount(str(test_device)).call()\n",
    "        print(f\"📊 Test device has {test_count} records\")\n",
    "        \n",
    "        if test_count > 0:\n",
    "            test_record = contract.functions.getDataByIndex(str(test_device), 0).call()\n",
    "            print(f\"🧪 Test record structure: {test_record}\")\n",
    "            print(f\"   Data Type: {test_record[0]}\")\n",
    "            print(f\"   Value: {test_record[1]}\")\n",
    "            print(f\"   Blockchain Timestamp: {test_record[2]} (will ignore this)\")\n",
    "            \n",
    "            # Show what CSV timestamp we'll use instead\n",
    "            test_csv_data = df_original[df_original[\"package_id\"] == test_device].iloc[0]\n",
    "            print(f\"   CSV Timestamp: {test_csv_data['timestamp']} (will use this)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "\n",
    "for device_id in device_ids:\n",
    "    try:\n",
    "        device_count = contract.functions.getDataCount(str(device_id)).call()\n",
    "        \n",
    "        if device_count > 0:\n",
    "            # Get original data for location/status info\n",
    "            device_original_data = df_original[df_original[\"package_id\"] == device_id].iloc[0]\n",
    "            \n",
    "            # Get each record for this device from blockchain\n",
    "            for i in range(device_count):\n",
    "                try:\n",
    "                    # Get record from blockchain: [dataType, value, timestamp]\n",
    "                    blockchain_record = contract.functions.getDataByIndex(str(device_id), i).call()\n",
    "                    \n",
    "                    # DEBUG: Print first few records\n",
    "                    if retrieved_count < 3:\n",
    "                        print(f\"🔍 Record {retrieved_count}: {blockchain_record}\")\n",
    "                        print(f\"   Using CSV timestamp: {device_original_data['timestamp']}\")\n",
    "                    \n",
    "                    # CORRECTED: Use ORIGINAL CSV timestamp, not blockchain timestamp\n",
    "                    # Get the original timestamp from CSV data for this device\n",
    "                    original_timestamp = device_original_data[\"timestamp\"]\n",
    "                    \n",
    "                    # Convert to proper datetime format\n",
    "                    try:\n",
    "                        timestamp = pd.to_datetime(original_timestamp)\n",
    "                    except Exception as e:\n",
    "                        # Fallback: use a reasonable timestamp\n",
    "                        base_time = datetime.now() - timedelta(days=7)\n",
    "                        timestamp = base_time + timedelta(minutes=retrieved_count)\n",
    "                        timestamp_errors += 1\n",
    "                        print(f\"⚠️ Timestamp conversion error for {device_id}: {e}\")\n",
    "                    \n",
    "                    # Structure data for Tableau/Charts\n",
    "                    record_data = {\n",
    "                        \"timestamp\": timestamp,  # ✅ Using ORIGINAL CSV timestamp\n",
    "                        \"device_id\": str(device_id),\n",
    "                        \"data_type\": blockchain_record[0],  # temperature/humidity/shock\n",
    "                        \"data_value\": blockchain_record[1],  # raw value as string\n",
    "                        \"location\": device_original_data.get(\"location\", \"Unknown\"),\n",
    "                        \"closest_city\": device_original_data.get(\"closest_city\", \"Unknown\"),\n",
    "                        \"status\": device_original_data.get(\"status\", \"In Transit\"),\n",
    "                        \"origin\": device_original_data.get(\"origin\", \"Unknown\"),\n",
    "                        \"destination\": device_original_data.get(\"destination\", \"Unknown\")\n",
    "                    }\n",
    "                    \n",
    "                    # DEBUG: Print first record structure\n",
    "                    if retrieved_count == 0:\n",
    "                        print(f\"🧪 First record structure: {record_data}\")\n",
    "                    \n",
    "                    data.append(record_data)\n",
    "                    retrieved_count += 1\n",
    "                    \n",
    "                    # Show progress\n",
    "                    if retrieved_count % 100 == 0:\n",
    "                        print(f\"📥 Retrieved {retrieved_count}/{total_records} records...\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error retrieving record {i} for {device_id}: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing device {device_id}: {e}\")\n",
    "\n",
    "print(f\"✅ Retrieved {len(data)} records from blockchain!\")\n",
    "print(f\"📊 Data list length: {len(data)}\")\n",
    "if len(data) > 0:\n",
    "    print(f\"🔍 Sample data item keys: {list(data[0].keys())}\")\n",
    "    print(f\"🕐 Using original CSV timeline: {data[0]['timestamp']}\")\n",
    "if timestamp_errors > 0:\n",
    "    print(f\"⚠️ Fixed {timestamp_errors} timestamp conversion errors\")\n",
    "\n",
    "# Step 4: PROCESS DATA FOR VISUALIZATION\n",
    "print(f\"\\n🔧 Processing data for Tableau and line charts...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DEBUG: Check what we actually got\n",
    "print(f\"📊 DataFrame shape: {df.shape}\")\n",
    "if len(df) > 0:\n",
    "    print(f\"📋 Columns found: {list(df.columns)}\")\n",
    "    print(f\"📝 First record sample:\")\n",
    "    print(df.head(1).to_dict('records'))\n",
    "else:\n",
    "    print(\"❌ No data retrieved! Check blockchain connection.\")\n",
    "    exit()\n",
    "\n",
    "# Check if DataFrame has the expected columns\n",
    "expected_columns = ['timestamp', 'device_id', 'data_type', 'data_value']\n",
    "missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"❌ Missing columns in DataFrame: {missing_columns}\")\n",
    "    print(f\"📋 Available columns: {list(df.columns)}\")\n",
    "    print(\"🔍 Sample data structure:\")\n",
    "    if len(data) > 0:\n",
    "        print(data[0])\n",
    "    exit()\n",
    "\n",
    "# Ensure proper timestamp format\n",
    "print(\"🕐 Converting timestamps...\")\n",
    "try:\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    print(\"✅ Timestamps converted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Timestamp conversion failed: {e}\")\n",
    "    print(\"🔍 Sample timestamp values:\")\n",
    "    print(df[\"timestamp\"].head())\n",
    "    exit()\n",
    "\n",
    "# Extract numeric values with better parsing\n",
    "print(\"🔢 Extracting numeric values...\")\n",
    "print(\"🛡️ Enforcing positive values only (IoT sensors don't produce negative readings)\")\n",
    "\n",
    "def extract_numeric(value_str):\n",
    "    \"\"\"Extract numeric value from string, handling various formats - NO NEGATIVE VALUES\n",
    "    \n",
    "    IoT sensors for temperature, humidity, and shock should not produce negative readings:\n",
    "    - Temperature: Absolute zero is 0°C minimum in normal conditions\n",
    "    - Humidity: Cannot be below 0% \n",
    "    - Shock: Measured as magnitude (always positive)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove common units and convert to float\n",
    "        clean_value = str(value_str).replace('°C', '').replace('%', '').replace('g', '').strip()\n",
    "        # Extract first number found - EXCLUDE negative numbers\n",
    "        import re\n",
    "        numbers = re.findall(r'\\d+\\.?\\d*', clean_value)  # Removed '-?' to prevent negative\n",
    "        if numbers:\n",
    "            value = float(numbers[0])\n",
    "            # Ensure no negative values - convert to absolute value\n",
    "            return abs(value)\n",
    "        return 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "df[\"numeric_value\"] = df[\"data_value\"].apply(extract_numeric)\n",
    "\n",
    "# Additional validation: Ensure NO negative values anywhere\n",
    "negative_count = (df[\"numeric_value\"] < 0).sum()\n",
    "if negative_count > 0:\n",
    "    print(f\"⚠️ Found {negative_count} negative values, converting to positive...\")\n",
    "    df[\"numeric_value\"] = df[\"numeric_value\"].abs()  # Use assignment instead of inplace\n",
    "    print(f\"✅ All negative values converted to positive\")\n",
    "\n",
    "# Data validation and cleaning\n",
    "print(\"🧹 Cleaning and validating data...\")\n",
    "\n",
    "# Remove any completely invalid records\n",
    "initial_count = len(df)\n",
    "df = df[df[\"data_type\"].isin([\"temperature\", \"humidity\", \"shock\"])]\n",
    "df = df[df[\"numeric_value\"].notna()]\n",
    "\n",
    "# STRICT VALIDATION: No negative values allowed\n",
    "negative_values = df[\"numeric_value\"] < 0\n",
    "if negative_values.sum() > 0:\n",
    "    print(f\"⚠️ Found {negative_values.sum()} negative values, converting to absolute values...\")\n",
    "    df.loc[negative_values, \"numeric_value\"] = df.loc[negative_values, \"numeric_value\"].abs()\n",
    "\n",
    "# Additional sensor-specific validation\n",
    "print(\"🔍 Applying sensor-specific validation...\")\n",
    "\n",
    "# Temperature validation (reasonable range: 0-60°C)\n",
    "temp_data = df[\"data_type\"] == \"temperature\"\n",
    "extreme_temp = (df[\"numeric_value\"] > 60) & temp_data\n",
    "if extreme_temp.sum() > 0:\n",
    "    print(f\"⚠️ Found {extreme_temp.sum()} extreme temperature values (>60°C), capping at 60°C\")\n",
    "    df.loc[extreme_temp, \"numeric_value\"] = 60\n",
    "\n",
    "# Humidity validation (reasonable range: 0-100%)\n",
    "humidity_data = df[\"data_type\"] == \"humidity\"\n",
    "extreme_humidity = (df[\"numeric_value\"] > 100) & humidity_data\n",
    "if extreme_humidity.sum() > 0:\n",
    "    print(f\"⚠️ Found {extreme_humidity.sum()} extreme humidity values (>100%), capping at 100%\")\n",
    "    df.loc[extreme_humidity, \"numeric_value\"] = 100\n",
    "\n",
    "# Shock validation (reasonable range: 0-10g)\n",
    "shock_data = df[\"data_type\"] == \"shock\"\n",
    "extreme_shock = (df[\"numeric_value\"] > 10) & shock_data\n",
    "if extreme_shock.sum() > 0:\n",
    "    print(f\"⚠️ Found {extreme_shock.sum()} extreme shock values (>10g), capping at 10g\")\n",
    "    df.loc[extreme_shock, \"numeric_value\"] = 10\n",
    "\n",
    "# Final validation: Ensure ALL values are positive\n",
    "final_negative_check = (df[\"numeric_value\"] < 0).sum()\n",
    "if final_negative_check > 0:\n",
    "    print(f\"❌ Still found {final_negative_check} negative values after cleaning!\")\n",
    "    df[\"numeric_value\"] = df[\"numeric_value\"].abs()  # Use assignment instead of inplace\n",
    "    print(\"✅ Forced all values to positive\")\n",
    "\n",
    "final_count = len(df)\n",
    "\n",
    "if final_count < initial_count:\n",
    "    print(f\"⚠️ Removed {initial_count - final_count} invalid records\")\n",
    "\n",
    "# Handle any remaining missing values with positive defaults\n",
    "df = df.fillna({\"numeric_value\": 0})  # Fix for pandas FutureWarning\n",
    "df = df.fillna(\"Unknown\")  # Fill other missing values\n",
    "\n",
    "# Final check: Confirm no negative values exist\n",
    "min_value = df[\"numeric_value\"].min()\n",
    "if min_value < 0:\n",
    "    print(f\"❌ ERROR: Still have negative values! Minimum: {min_value}\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"✅ All numeric values are positive. Minimum value: {min_value:.2f}\")\n",
    "\n",
    "# Add record ID for Tableau (helps with aggregation issues)\n",
    "df[\"record_id\"] = range(1, len(df) + 1)\n",
    "\n",
    "# Sort by timestamp for proper line chart display\n",
    "df = df.sort_values([\"timestamp\", \"device_id\", \"data_type\"]).reset_index(drop=True)\n",
    "\n",
    "# Step 5: DATA QUALITY VALIDATION\n",
    "print(f\"\\n📊 Data Quality Report:\")\n",
    "print(f\"   ✅ Total records: {len(df)}\")\n",
    "print(f\"   📦 Unique packages: {df['device_id'].nunique()}\")\n",
    "print(f\"   📈 Sensor types: {', '.join(sorted(df['data_type'].unique()))}\")\n",
    "print(f\"   🌍 Cities: {df['closest_city'].nunique()}\")\n",
    "print(f\"   📅 Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "\n",
    "# Sensor data validation\n",
    "print(f\"\\n🔍 Sensor Data Validation (ALL VALUES POSITIVE):\")\n",
    "sensor_stats = {}\n",
    "\n",
    "for sensor_type in [\"temperature\", \"humidity\", \"shock\"]:\n",
    "    sensor_data = df[df['data_type'] == sensor_type]['numeric_value']\n",
    "    if len(sensor_data) > 0:\n",
    "        stats = {\n",
    "            'count': len(sensor_data),\n",
    "            'min': sensor_data.min(),\n",
    "            'max': sensor_data.max(), \n",
    "            'mean': sensor_data.mean(),\n",
    "            'std': sensor_data.std()\n",
    "        }\n",
    "        sensor_stats[sensor_type] = stats\n",
    "        print(f\"   {sensor_type}: {stats['count']} readings\")\n",
    "        print(f\"     Range: {stats['min']:.2f} - {stats['max']:.2f}\")\n",
    "        print(f\"     Average: {stats['mean']:.2f} ± {stats['std']:.2f}\")\n",
    "        \n",
    "        # Validation: Ensure no negative values\n",
    "        if stats['min'] < 0:\n",
    "            print(f\"     ❌ ERROR: Found negative values in {sensor_type}!\")\n",
    "        else:\n",
    "            print(f\"     ✅ All {sensor_type} values are positive\")\n",
    "\n",
    "# Data distribution check\n",
    "print(f\"\\n📊 Records per sensor type:\")\n",
    "for sensor in sorted(df['data_type'].unique()):\n",
    "    count = len(df[df['data_type'] == sensor])\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   {sensor}: {count} records ({percentage:.1f}%)\")\n",
    "\n",
    "# Timeline validation\n",
    "timeline_span = df['timestamp'].max() - df['timestamp'].min()\n",
    "print(f\"\\n⏰ Timeline validation (using ORIGINAL CSV timeline):\")\n",
    "print(f\"   Time span: {timeline_span}\")\n",
    "print(f\"   Records per day: {len(df) / max(1, timeline_span.days):.1f}\")\n",
    "print(f\"   📅 Data covers: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "\n",
    "# Step 6: SAVE CLEAN DATASET FOR TABLEAU/CHARTS\n",
    "print(f\"\\n💾 Saving dataset optimized for Tableau and line charts...\")\n",
    "\n",
    "# Create the final clean dataset\n",
    "final_columns = [\n",
    "    'record_id',        # Unique identifier\n",
    "    'timestamp',        # Properly formatted datetime\n",
    "    'device_id',        # Package identifier  \n",
    "    'data_type',        # Sensor type (temperature/humidity/shock)\n",
    "    'data_value',       # Original string value\n",
    "    'numeric_value',    # Cleaned numeric value\n",
    "    'location',         # GPS coordinates\n",
    "    'closest_city',     # City name\n",
    "    'status',           # Package status\n",
    "    'origin',           # Origin city\n",
    "    'destination'       # Destination city\n",
    "]\n",
    "\n",
    "df_final = df[final_columns].copy()\n",
    "\n",
    "# Save main dataset\n",
    "try:\n",
    "    df_final.to_csv(\"cleaned_iot_data.csv\", index=False)\n",
    "    print(\"✅ Main dataset saved: cleaned_iot_data.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving main dataset: {e}\")\n",
    "\n",
    "# Save sensor-specific datasets for individual analysis\n",
    "try:\n",
    "    for sensor_type in [\"temperature\", \"humidity\", \"shock\"]:\n",
    "        sensor_df = df_final[df_final['data_type'] == sensor_type].copy()\n",
    "        filename = f\"cleaned_{sensor_type}_data.csv\"\n",
    "        sensor_df.to_csv(filename, index=False)\n",
    "        print(f\"✅ {sensor_type.title()} dataset saved: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving sensor datasets: {e}\")\n",
    "\n",
    "# Save summary statistics\n",
    "try:\n",
    "    summary_data = []\n",
    "    for sensor_type, stats in sensor_stats.items():\n",
    "        summary_data.append({\n",
    "            'sensor_type': sensor_type,\n",
    "            'total_records': stats['count'],\n",
    "            'min_value': stats['min'],\n",
    "            'max_value': stats['max'],\n",
    "            'avg_value': stats['mean'],\n",
    "            'std_deviation': stats['std']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(\"iot_data_summary.csv\", index=False)\n",
    "    print(\"✅ Summary statistics saved: iot_data_summary.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving summary: {e}\")\n",
    "\n",
    "# Step 7: TABLEAU PREPARATION NOTES\n",
    "print(f\"\\n📋 TABLEAU PREPARATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"📁 FILES CREATED:\")\n",
    "print(\"   cleaned_iot_data.csv - Main dataset for Tableau dashboard\")\n",
    "print(\"   cleaned_temperature_data.csv - Temperature sensor only\")  \n",
    "print(\"   cleaned_humidity_data.csv - Humidity sensor only\")\n",
    "print(\"   cleaned_shock_data.csv - Shock sensor only\")\n",
    "print(\"   iot_data_summary.csv - Statistical summary\")\n",
    "\n",
    "print(f\"\\n🎉 SUCCESS! Dataset ready for visualization!\")\n",
    "print(f\"📊 Total: {len(df_final)} records from {df_final['device_id'].nunique()} packages\")\n",
    "print(f\"🕐 Original timeline preserved: {df_final['timestamp'].min()} to {df_final['timestamp'].max()}\")\n",
    "print(f\"✅ ALL VALUES POSITIVE - No negative readings in dataset\")\n",
    "print(\"🚀 Ready for Tableau dashboard and line chart creation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
