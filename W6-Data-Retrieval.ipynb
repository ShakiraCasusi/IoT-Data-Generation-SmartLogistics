{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f68364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Starting Week 6: Data Retrieval and Processing (CORRECTED VERSION)\n",
      "üéØ Output: Clean dataset ready for Tableau dashboards and line charts\n",
      "======================================================================\n",
      "üîó Connecting to blockchain...\n",
      "‚úÖ Connected to Ganache successfully!\n",
      "‚úÖ Connected to Smart Contract\n",
      "\n",
      "üìÅ Looking for logistics data file...\n",
      "üìã CSV files found: ['cleaned_humidity_data.csv', 'cleaned_iot_data.csv', 'cleaned_shock_data.csv', 'cleaned_temperature_data.csv', 'iot_data_summary.csv', 'Logistics-Data.csv', 'W6-Cleaned_Iot_Data.csv']\n",
      "‚úÖ Found and loaded: Logistics-Data.csv\n",
      "üìä Data shape: (100, 10)\n",
      "‚úÖ All required columns found!\n",
      "\n",
      "üìä Checking blockchain data...\n",
      "üì¶ Total packages: 100\n",
      "üìà Blockchain records found: 300\n",
      "\n",
      "üì• Retrieving 300 records from blockchain...\n",
      "üïê Using ORIGINAL CSV timestamps (not blockchain timestamps)\n",
      "üìÖ Timeline will match your original logistics data\n",
      "üìä Original data timeline: 2025-04-29 12:08:30.336512 to 2025-05-06 10:09:30.336512\n",
      "üîç Testing first device: PKG85046\n",
      "üìä Test device has 3 records\n",
      "üß™ Test record structure: ['temperature', '26.0', 1750487450]\n",
      "   Data Type: temperature\n",
      "   Value: 26.0\n",
      "   Blockchain Timestamp: 1750487450 (will ignore this)\n",
      "   CSV Timestamp: 2025-04-29 12:08:30.336512 (will use this)\n",
      "üîç Record 0: ['temperature', '26.0', 1750487450]\n",
      "   Using CSV timestamp: 2025-04-29 12:08:30.336512\n",
      "üß™ First record structure: {'timestamp': Timestamp('2025-04-29 12:08:30.336512'), 'device_id': 'PKG85046', 'data_type': 'temperature', 'data_value': '26.0', 'location': '33.3012, 13.5857', 'closest_city': 'Cairo', 'status': 'In Transit', 'origin': 'S√£o Paulo', 'destination': 'Moscow'}\n",
      "üîç Record 1: ['humidity', '51.6', 1750487450]\n",
      "   Using CSV timestamp: 2025-04-29 12:08:30.336512\n",
      "üîç Record 2: ['shock', '0.2', 1750487450]\n",
      "   Using CSV timestamp: 2025-04-29 12:08:30.336512\n",
      "üì• Retrieved 100/300 records...\n",
      "üì• Retrieved 200/300 records...\n",
      "üì• Retrieved 300/300 records...\n",
      "‚úÖ Retrieved 300 records from blockchain!\n",
      "üìä Data list length: 300\n",
      "üîç Sample data item keys: ['timestamp', 'device_id', 'data_type', 'data_value', 'location', 'closest_city', 'status', 'origin', 'destination']\n",
      "üïê Using original CSV timeline: 2025-04-29 12:08:30.336512\n",
      "\n",
      "üîß Processing data for Tableau and line charts...\n",
      "üìä DataFrame shape: (300, 9)\n",
      "üìã Columns found: ['timestamp', 'device_id', 'data_type', 'data_value', 'location', 'closest_city', 'status', 'origin', 'destination']\n",
      "üìù First record sample:\n",
      "[{'timestamp': Timestamp('2025-04-29 12:08:30.336512'), 'device_id': 'PKG85046', 'data_type': 'temperature', 'data_value': '26.0', 'location': '33.3012, 13.5857', 'closest_city': 'Cairo', 'status': 'In Transit', 'origin': 'S√£o Paulo', 'destination': 'Moscow'}]\n",
      "üïê Converting timestamps...\n",
      "‚úÖ Timestamps converted successfully\n",
      "üî¢ Extracting numeric values...\n",
      "üõ°Ô∏è Enforcing positive values only (IoT sensors don't produce negative readings)\n",
      "üßπ Cleaning and validating data...\n",
      "üîç Applying sensor-specific validation...\n",
      "‚úÖ All numeric values are positive. Minimum value: 0.01\n",
      "\n",
      "üìä Data Quality Report:\n",
      "   ‚úÖ Total records: 300\n",
      "   üì¶ Unique packages: 100\n",
      "   üìà Sensor types: humidity, shock, temperature\n",
      "   üåç Cities: 19\n",
      "   üìÖ Date range: 2025-04-29 12:08:30.336512 to 2025-05-06 10:09:30.336512\n",
      "\n",
      "üîç Sensor Data Validation (ALL VALUES POSITIVE):\n",
      "   temperature: 100 readings\n",
      "     Range: 0.60 - 42.50\n",
      "     Average: 19.40 ¬± 9.93\n",
      "     ‚úÖ All temperature values are positive\n",
      "   humidity: 100 readings\n",
      "     Range: 41.70 - 96.00\n",
      "     Average: 74.05 ¬± 11.38\n",
      "     ‚úÖ All humidity values are positive\n",
      "   shock: 100 readings\n",
      "     Range: 0.01 - 1.98\n",
      "     Average: 0.30 ¬± 0.32\n",
      "     ‚úÖ All shock values are positive\n",
      "\n",
      "üìä Records per sensor type:\n",
      "   humidity: 100 records (33.3%)\n",
      "   shock: 100 records (33.3%)\n",
      "   temperature: 100 records (33.3%)\n",
      "\n",
      "‚è∞ Timeline validation (using ORIGINAL CSV timeline):\n",
      "   Time span: 6 days 22:01:00\n",
      "   Records per day: 50.0\n",
      "   üìÖ Data covers: 2025-04-29 12:08:30.336512 to 2025-05-06 10:09:30.336512\n",
      "\n",
      "üíæ Saving dataset optimized for Tableau and line charts...\n",
      "‚úÖ Main dataset saved: cleaned_iot_data.csv\n",
      "‚úÖ Temperature dataset saved: cleaned_temperature_data.csv\n",
      "‚úÖ Humidity dataset saved: cleaned_humidity_data.csv\n",
      "‚úÖ Shock dataset saved: cleaned_shock_data.csv\n",
      "‚úÖ Summary statistics saved: iot_data_summary.csv\n",
      "\n",
      "üìã TABLEAU PREPARATION COMPLETE!\n",
      "======================================================================\n",
      "üìÅ FILES CREATED:\n",
      "   cleaned_iot_data.csv - Main dataset for Tableau dashboard\n",
      "   cleaned_temperature_data.csv - Temperature sensor only\n",
      "   cleaned_humidity_data.csv - Humidity sensor only\n",
      "   cleaned_shock_data.csv - Shock sensor only\n",
      "   iot_data_summary.csv - Statistical summary\n",
      "\n",
      "üéâ SUCCESS! Dataset ready for visualization!\n",
      "üìä Total: 300 records from 100 packages\n",
      "üïê Original timeline preserved: 2025-04-29 12:08:30.336512 to 2025-05-06 10:09:30.336512\n",
      "‚úÖ ALL VALUES POSITIVE - No negative readings in dataset\n",
      "üöÄ Ready for Tableau dashboard and line chart creation!\n"
     ]
    }
   ],
   "source": [
    "from web3 import Web3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ============================================================================= \n",
    "# WEEK 6: CORRECTED DATA RETRIEVAL AND PROCESSING FOR TABLEAU/CHARTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üì• Starting Week 6: Data Retrieval and Processing (CORRECTED VERSION)\")\n",
    "print(\"üéØ Output: Clean dataset ready for Tableau dashboards and line charts\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Connect to Ganache blockchain\n",
    "print(\"üîó Connecting to blockchain...\")\n",
    "\n",
    "ganache_url = \"http://127.0.0.1:7545\"\n",
    "web3 = Web3(Web3.HTTPProvider(ganache_url))\n",
    "\n",
    "if web3.is_connected():\n",
    "    print(\"‚úÖ Connected to Ganache successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Connection failed. Make sure Ganache is running.\")\n",
    "    exit()\n",
    "\n",
    "# Contract details\n",
    "contract_address = web3.to_checksum_address(\"0x327ef98ba37138026c79783e02d2a752ecd58f9e\")\n",
    "contract_abi = [\n",
    "    {\n",
    "        \"inputs\": [{\"internalType\": \"address\",\"name\": \"device\",\"type\": \"address\"}],\n",
    "        \"name\": \"authorizeDevice\",\"outputs\": [],\"stateMutability\": \"nonpayable\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [{\"internalType\": \"address\",\"name\": \"device\",\"type\": \"address\"}],\n",
    "        \"name\": \"revokeDevice\",\"outputs\": [],\"stateMutability\": \"nonpayable\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            {\"internalType\": \"string\",\"name\": \"deviceID\",\"type\": \"string\"},\n",
    "            {\"internalType\": \"string\",\"name\": \"dataType\",\"type\": \"string\"},\n",
    "            {\"internalType\": \"string\",\"name\": \"value\",\"type\": \"string\"}\n",
    "        ],\n",
    "        \"name\": \"storeData\",\"outputs\": [],\"stateMutability\": \"nonpayable\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [],\"stateMutability\": \"nonpayable\",\"type\": \"constructor\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [{\"internalType\": \"address\",\"name\": \"\",\"type\": \"address\"}],\n",
    "        \"name\": \"authorizedDevices\",\"outputs\": [{\"internalType\": \"bool\",\"name\": \"\",\"type\": \"bool\"}],\n",
    "        \"stateMutability\": \"view\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            {\"internalType\": \"string\",\"name\": \"deviceID\",\"type\": \"string\"},\n",
    "            {\"internalType\": \"uint256\",\"name\": \"index\",\"type\": \"uint256\"}\n",
    "        ],\n",
    "        \"name\": \"getDataByIndex\",\n",
    "        \"outputs\": [\n",
    "            {\"internalType\": \"string\",\"name\": \"\",\"type\": \"string\"},\n",
    "            {\"internalType\": \"string\",\"name\": \"\",\"type\": \"string\"},\n",
    "            {\"internalType\": \"uint256\",\"name\": \"\",\"type\": \"uint256\"}\n",
    "        ],\n",
    "        \"stateMutability\": \"view\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [{\"internalType\": \"string\",\"name\": \"deviceID\",\"type\": \"string\"}],\n",
    "        \"name\": \"getDataCount\",\n",
    "        \"outputs\": [{\"internalType\": \"uint256\",\"name\": \"\",\"type\": \"uint256\"}],\n",
    "        \"stateMutability\": \"view\",\"type\": \"function\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": [],\"name\": \"owner\",\n",
    "        \"outputs\": [{\"internalType\": \"address\",\"name\": \"\",\"type\": \"address\"}],\n",
    "        \"stateMutability\": \"view\",\"type\": \"function\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Load the smart contract\n",
    "contract = web3.eth.contract(address=contract_address, abi=contract_abi)\n",
    "web3.eth.default_account = web3.eth.accounts[0]\n",
    "print(\"‚úÖ Connected to Smart Contract\")\n",
    "\n",
    "# Step 2: Find and load CSV file for reference data\n",
    "print(\"\\nüìÅ Looking for logistics data file...\")\n",
    "\n",
    "possible_files = [\n",
    "    \"LogisticsData.csv\",\n",
    "    \"Logistics-Data.csv\", \n",
    "    \"logistics_data.csv\",\n",
    "    \"logistics-data.csv\",\n",
    "    \"IoT_Data.csv\",\n",
    "    \"iot_data.csv\"\n",
    "]\n",
    "\n",
    "df_original = None\n",
    "csv_filename = None\n",
    "\n",
    "# Check current directory for CSV files\n",
    "current_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "print(f\"üìã CSV files found: {current_files}\")\n",
    "\n",
    "# Try to find the logistics data file\n",
    "for filename in possible_files:\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            df_original = pd.read_csv(filename)\n",
    "            csv_filename = filename\n",
    "            print(f\"‚úÖ Found and loaded: {filename}\")\n",
    "            print(f\"üìä Data shape: {df_original.shape}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {filename}: {e}\")\n",
    "\n",
    "# If still no file found, try the first CSV file in directory\n",
    "if df_original is None and current_files:\n",
    "    try:\n",
    "        csv_filename = current_files[0]\n",
    "        df_original = pd.read_csv(csv_filename)\n",
    "        print(f\"‚úÖ Using first available CSV: {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {csv_filename}: {e}\")\n",
    "\n",
    "if df_original is None:\n",
    "    print(\"‚ùå No logistics data file found. Please ensure you have the logistics CSV file.\")\n",
    "    exit()\n",
    "\n",
    "# Verify required columns exist\n",
    "required_columns = ['package_id', 'temperature', 'humidity', 'shock']\n",
    "missing_columns = [col for col in required_columns if col not in df_original.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"‚ùå Missing required columns: {missing_columns}\")\n",
    "    print(\"üìã Available columns:\", list(df_original.columns))\n",
    "    exit()\n",
    "\n",
    "print(\"‚úÖ All required columns found!\")\n",
    "\n",
    "# Step 3: Get blockchain data count\n",
    "print(\"\\nüìä Checking blockchain data...\")\n",
    "\n",
    "device_ids = df_original[\"package_id\"].unique()\n",
    "total_records = 0\n",
    "\n",
    "for device_id in device_ids:\n",
    "    try:\n",
    "        device_count = contract.functions.getDataCount(str(device_id)).call()\n",
    "        total_records += device_count\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error checking records for {device_id}: {e}\")\n",
    "\n",
    "print(f\"üì¶ Total packages: {len(device_ids)}\")\n",
    "print(f\"üìà Blockchain records found: {total_records}\")\n",
    "\n",
    "if total_records == 0:\n",
    "    print(\"‚ö†Ô∏è No data on blockchain!\")\n",
    "    print(\"üöÄ Will automatically store CSV data to blockchain first...\")\n",
    "    \n",
    "    # STORE ALL DATA TO BLOCKCHAIN\n",
    "    print(f\"\\nüì§ Storing IoT data to blockchain...\")\n",
    "    \n",
    "    # Authorize the current account first\n",
    "    try:\n",
    "        print(\"üîë Authorizing device...\")\n",
    "        auth_txn = contract.functions.authorizeDevice(web3.eth.default_account).transact({\n",
    "            'from': web3.eth.default_account,\n",
    "            'gas': 3000000\n",
    "        })\n",
    "        web3.eth.wait_for_transaction_receipt(auth_txn)\n",
    "        print(\"‚úÖ Device authorized successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Authorization issue (may already exist): {e}\")\n",
    "    \n",
    "    # Store ALL data from CSV to blockchain\n",
    "    stored_count = 0\n",
    "    failed_count = 0\n",
    "    expected_records = len(df_original) * 3  # 3 sensors per package\n",
    "    \n",
    "    print(f\"üì¶ Storing data from {len(df_original)} packages...\")\n",
    "    print(f\"üéØ Will create {expected_records} blockchain records...\")\n",
    "    \n",
    "    for index, row in df_original.iterrows():\n",
    "        device_id = str(row[\"package_id\"])\n",
    "        \n",
    "        # Store temperature, humidity, and shock data with validation\n",
    "        sensors = [\n",
    "            (\"temperature\", abs(float(row[\"temperature\"]))),  # Ensure positive\n",
    "            (\"humidity\", abs(float(row[\"humidity\"]))),        # Ensure positive\n",
    "            (\"shock\", abs(float(row[\"shock\"])))               # Ensure positive\n",
    "        ]\n",
    "        \n",
    "        # Show progress every 10 packages\n",
    "        if index % 10 == 0:\n",
    "            print(f\"üìä Progress: {index+1}/{len(df_original)} packages ({stored_count} records stored)...\")\n",
    "        \n",
    "        for sensor_type, sensor_value in sensors:\n",
    "            # Additional validation: Ensure reasonable ranges\n",
    "            if sensor_type == \"temperature\" and sensor_value > 60:\n",
    "                sensor_value = 60  # Cap extreme temperatures\n",
    "            elif sensor_type == \"humidity\" and sensor_value > 100:\n",
    "                sensor_value = 100  # Cap humidity at 100%\n",
    "            elif sensor_type == \"shock\" and sensor_value > 10:\n",
    "                sensor_value = 10  # Cap extreme shock values\n",
    "            \n",
    "            try:\n",
    "                txn = contract.functions.storeData(device_id, sensor_type, str(sensor_value)).transact({\n",
    "                    'from': web3.eth.default_account,\n",
    "                    'gas': 3000000\n",
    "                })\n",
    "                web3.eth.wait_for_transaction_receipt(txn)\n",
    "                stored_count += 1\n",
    "                \n",
    "                # Show detailed progress for first few records\n",
    "                if stored_count <= 5:\n",
    "                    print(f\"   üõ∞Ô∏è Stored [{device_id}] - {sensor_type}: {sensor_value}\")\n",
    "                \n",
    "                # Small delay to prevent overwhelming Ganache\n",
    "                time.sleep(0.05)\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                print(f\"   ‚ùå Failed to store {sensor_type} for {device_id}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Data storage completed!\")\n",
    "    print(f\"‚úÖ Successfully stored: {stored_count} records\")\n",
    "    print(f\"‚ùå Failed to store: {failed_count} records\")\n",
    "    if stored_count + failed_count > 0:\n",
    "        print(f\"üìä Success rate: {(stored_count/(stored_count+failed_count)*100):.1f}%\")\n",
    "    \n",
    "    # Update total_records for retrieval\n",
    "    total_records = stored_count\n",
    "    print(f\"üìà Blockchain now contains {total_records} records\")\n",
    "    \n",
    "    if total_records == 0:\n",
    "        print(\"‚ùå No data was stored successfully. Check blockchain connection.\")\n",
    "        exit()\n",
    "\n",
    "# NOW RETRIEVE THE DATA FROM BLOCKCHAIN\n",
    "print(f\"\\nüì• Retrieving {total_records} records from blockchain...\")\n",
    "print(\"üïê Using ORIGINAL CSV timestamps (not blockchain timestamps)\")\n",
    "print(\"üìÖ Timeline will match your original logistics data\")\n",
    "\n",
    "data = []\n",
    "retrieved_count = 0\n",
    "timestamp_errors = 0\n",
    "\n",
    "# Show expected timestamp range from CSV\n",
    "csv_timestamps = pd.to_datetime(df_original[\"timestamp\"])\n",
    "print(f\"üìä Original data timeline: {csv_timestamps.min()} to {csv_timestamps.max()}\")\n",
    "\n",
    "# Test first device to debug\n",
    "if len(device_ids) > 0:\n",
    "    test_device = device_ids[0]\n",
    "    print(f\"üîç Testing first device: {test_device}\")\n",
    "    try:\n",
    "        test_count = contract.functions.getDataCount(str(test_device)).call()\n",
    "        print(f\"üìä Test device has {test_count} records\")\n",
    "        \n",
    "        if test_count > 0:\n",
    "            test_record = contract.functions.getDataByIndex(str(test_device), 0).call()\n",
    "            print(f\"üß™ Test record structure: {test_record}\")\n",
    "            print(f\"   Data Type: {test_record[0]}\")\n",
    "            print(f\"   Value: {test_record[1]}\")\n",
    "            print(f\"   Blockchain Timestamp: {test_record[2]} (will ignore this)\")\n",
    "            \n",
    "            # Show what CSV timestamp we'll use instead\n",
    "            test_csv_data = df_original[df_original[\"package_id\"] == test_device].iloc[0]\n",
    "            print(f\"   CSV Timestamp: {test_csv_data['timestamp']} (will use this)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")\n",
    "\n",
    "for device_id in device_ids:\n",
    "    try:\n",
    "        device_count = contract.functions.getDataCount(str(device_id)).call()\n",
    "        \n",
    "        if device_count > 0:\n",
    "            # Get original data for location/status info\n",
    "            device_original_data = df_original[df_original[\"package_id\"] == device_id].iloc[0]\n",
    "            \n",
    "            # Get each record for this device from blockchain\n",
    "            for i in range(device_count):\n",
    "                try:\n",
    "                    # Get record from blockchain: [dataType, value, timestamp]\n",
    "                    blockchain_record = contract.functions.getDataByIndex(str(device_id), i).call()\n",
    "                    \n",
    "                    # DEBUG: Print first few records\n",
    "                    if retrieved_count < 3:\n",
    "                        print(f\"üîç Record {retrieved_count}: {blockchain_record}\")\n",
    "                        print(f\"   Using CSV timestamp: {device_original_data['timestamp']}\")\n",
    "                    \n",
    "                    # CORRECTED: Use ORIGINAL CSV timestamp, not blockchain timestamp\n",
    "                    # Get the original timestamp from CSV data for this device\n",
    "                    original_timestamp = device_original_data[\"timestamp\"]\n",
    "                    \n",
    "                    # Convert to proper datetime format\n",
    "                    try:\n",
    "                        timestamp = pd.to_datetime(original_timestamp)\n",
    "                    except Exception as e:\n",
    "                        # Fallback: use a reasonable timestamp\n",
    "                        base_time = datetime.now() - timedelta(days=7)\n",
    "                        timestamp = base_time + timedelta(minutes=retrieved_count)\n",
    "                        timestamp_errors += 1\n",
    "                        print(f\"‚ö†Ô∏è Timestamp conversion error for {device_id}: {e}\")\n",
    "                    \n",
    "                    # Structure data for Tableau/Charts\n",
    "                    record_data = {\n",
    "                        \"timestamp\": timestamp,  # ‚úÖ Using ORIGINAL CSV timestamp\n",
    "                        \"device_id\": str(device_id),\n",
    "                        \"data_type\": blockchain_record[0],  # temperature/humidity/shock\n",
    "                        \"data_value\": blockchain_record[1],  # raw value as string\n",
    "                        \"location\": device_original_data.get(\"location\", \"Unknown\"),\n",
    "                        \"closest_city\": device_original_data.get(\"closest_city\", \"Unknown\"),\n",
    "                        \"status\": device_original_data.get(\"status\", \"In Transit\"),\n",
    "                        \"origin\": device_original_data.get(\"origin\", \"Unknown\"),\n",
    "                        \"destination\": device_original_data.get(\"destination\", \"Unknown\")\n",
    "                    }\n",
    "                    \n",
    "                    # DEBUG: Print first record structure\n",
    "                    if retrieved_count == 0:\n",
    "                        print(f\"üß™ First record structure: {record_data}\")\n",
    "                    \n",
    "                    data.append(record_data)\n",
    "                    retrieved_count += 1\n",
    "                    \n",
    "                    # Show progress\n",
    "                    if retrieved_count % 100 == 0:\n",
    "                        print(f\"üì• Retrieved {retrieved_count}/{total_records} records...\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error retrieving record {i} for {device_id}: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing device {device_id}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Retrieved {len(data)} records from blockchain!\")\n",
    "print(f\"üìä Data list length: {len(data)}\")\n",
    "if len(data) > 0:\n",
    "    print(f\"üîç Sample data item keys: {list(data[0].keys())}\")\n",
    "    print(f\"üïê Using original CSV timeline: {data[0]['timestamp']}\")\n",
    "if timestamp_errors > 0:\n",
    "    print(f\"‚ö†Ô∏è Fixed {timestamp_errors} timestamp conversion errors\")\n",
    "\n",
    "# Step 4: PROCESS DATA FOR VISUALIZATION\n",
    "print(f\"\\nüîß Processing data for Tableau and line charts...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DEBUG: Check what we actually got\n",
    "print(f\"üìä DataFrame shape: {df.shape}\")\n",
    "if len(df) > 0:\n",
    "    print(f\"üìã Columns found: {list(df.columns)}\")\n",
    "    print(f\"üìù First record sample:\")\n",
    "    print(df.head(1).to_dict('records'))\n",
    "else:\n",
    "    print(\"‚ùå No data retrieved! Check blockchain connection.\")\n",
    "    exit()\n",
    "\n",
    "# Check if DataFrame has the expected columns\n",
    "expected_columns = ['timestamp', 'device_id', 'data_type', 'data_value']\n",
    "missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"‚ùå Missing columns in DataFrame: {missing_columns}\")\n",
    "    print(f\"üìã Available columns: {list(df.columns)}\")\n",
    "    print(\"üîç Sample data structure:\")\n",
    "    if len(data) > 0:\n",
    "        print(data[0])\n",
    "    exit()\n",
    "\n",
    "# Ensure proper timestamp format\n",
    "print(\"üïê Converting timestamps...\")\n",
    "try:\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    print(\"‚úÖ Timestamps converted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Timestamp conversion failed: {e}\")\n",
    "    print(\"üîç Sample timestamp values:\")\n",
    "    print(df[\"timestamp\"].head())\n",
    "    exit()\n",
    "\n",
    "# Extract numeric values with better parsing\n",
    "print(\"üî¢ Extracting numeric values...\")\n",
    "print(\"üõ°Ô∏è Enforcing positive values only (IoT sensors don't produce negative readings)\")\n",
    "\n",
    "def extract_numeric(value_str):\n",
    "    \"\"\"Extract numeric value from string, handling various formats - NO NEGATIVE VALUES\n",
    "    \n",
    "    IoT sensors for temperature, humidity, and shock should not produce negative readings:\n",
    "    - Temperature: Absolute zero is 0¬∞C minimum in normal conditions\n",
    "    - Humidity: Cannot be below 0% \n",
    "    - Shock: Measured as magnitude (always positive)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove common units and convert to float\n",
    "        clean_value = str(value_str).replace('¬∞C', '').replace('%', '').replace('g', '').strip()\n",
    "        # Extract first number found - EXCLUDE negative numbers\n",
    "        import re\n",
    "        numbers = re.findall(r'\\d+\\.?\\d*', clean_value)  # Removed '-?' to prevent negative\n",
    "        if numbers:\n",
    "            value = float(numbers[0])\n",
    "            # Ensure no negative values - convert to absolute value\n",
    "            return abs(value)\n",
    "        return 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "df[\"numeric_value\"] = df[\"data_value\"].apply(extract_numeric)\n",
    "\n",
    "# Additional validation: Ensure NO negative values anywhere\n",
    "negative_count = (df[\"numeric_value\"] < 0).sum()\n",
    "if negative_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {negative_count} negative values, converting to positive...\")\n",
    "    df[\"numeric_value\"] = df[\"numeric_value\"].abs()  # Use assignment instead of inplace\n",
    "    print(f\"‚úÖ All negative values converted to positive\")\n",
    "\n",
    "# Data validation and cleaning\n",
    "print(\"üßπ Cleaning and validating data...\")\n",
    "\n",
    "# Remove any completely invalid records\n",
    "initial_count = len(df)\n",
    "df = df[df[\"data_type\"].isin([\"temperature\", \"humidity\", \"shock\"])]\n",
    "df = df[df[\"numeric_value\"].notna()]\n",
    "\n",
    "# STRICT VALIDATION: No negative values allowed\n",
    "negative_values = df[\"numeric_value\"] < 0\n",
    "if negative_values.sum() > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {negative_values.sum()} negative values, converting to absolute values...\")\n",
    "    df.loc[negative_values, \"numeric_value\"] = df.loc[negative_values, \"numeric_value\"].abs()\n",
    "\n",
    "# Additional sensor-specific validation\n",
    "print(\"üîç Applying sensor-specific validation...\")\n",
    "\n",
    "# Temperature validation (reasonable range: 0-60¬∞C)\n",
    "temp_data = df[\"data_type\"] == \"temperature\"\n",
    "extreme_temp = (df[\"numeric_value\"] > 60) & temp_data\n",
    "if extreme_temp.sum() > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {extreme_temp.sum()} extreme temperature values (>60¬∞C), capping at 60¬∞C\")\n",
    "    df.loc[extreme_temp, \"numeric_value\"] = 60\n",
    "\n",
    "# Humidity validation (reasonable range: 0-100%)\n",
    "humidity_data = df[\"data_type\"] == \"humidity\"\n",
    "extreme_humidity = (df[\"numeric_value\"] > 100) & humidity_data\n",
    "if extreme_humidity.sum() > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {extreme_humidity.sum()} extreme humidity values (>100%), capping at 100%\")\n",
    "    df.loc[extreme_humidity, \"numeric_value\"] = 100\n",
    "\n",
    "# Shock validation (reasonable range: 0-10g)\n",
    "shock_data = df[\"data_type\"] == \"shock\"\n",
    "extreme_shock = (df[\"numeric_value\"] > 10) & shock_data\n",
    "if extreme_shock.sum() > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {extreme_shock.sum()} extreme shock values (>10g), capping at 10g\")\n",
    "    df.loc[extreme_shock, \"numeric_value\"] = 10\n",
    "\n",
    "# Final validation: Ensure ALL values are positive\n",
    "final_negative_check = (df[\"numeric_value\"] < 0).sum()\n",
    "if final_negative_check > 0:\n",
    "    print(f\"‚ùå Still found {final_negative_check} negative values after cleaning!\")\n",
    "    df[\"numeric_value\"] = df[\"numeric_value\"].abs()  # Use assignment instead of inplace\n",
    "    print(\"‚úÖ Forced all values to positive\")\n",
    "\n",
    "final_count = len(df)\n",
    "\n",
    "if final_count < initial_count:\n",
    "    print(f\"‚ö†Ô∏è Removed {initial_count - final_count} invalid records\")\n",
    "\n",
    "# Handle any remaining missing values with positive defaults\n",
    "df = df.fillna({\"numeric_value\": 0})  # Fix for pandas FutureWarning\n",
    "df = df.fillna(\"Unknown\")  # Fill other missing values\n",
    "\n",
    "# Final check: Confirm no negative values exist\n",
    "min_value = df[\"numeric_value\"].min()\n",
    "if min_value < 0:\n",
    "    print(f\"‚ùå ERROR: Still have negative values! Minimum: {min_value}\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"‚úÖ All numeric values are positive. Minimum value: {min_value:.2f}\")\n",
    "\n",
    "# Add record ID for Tableau (helps with aggregation issues)\n",
    "df[\"record_id\"] = range(1, len(df) + 1)\n",
    "\n",
    "# Sort by timestamp for proper line chart display\n",
    "df = df.sort_values([\"timestamp\", \"device_id\", \"data_type\"]).reset_index(drop=True)\n",
    "\n",
    "# Step 5: DATA QUALITY VALIDATION\n",
    "print(f\"\\nüìä Data Quality Report:\")\n",
    "print(f\"   ‚úÖ Total records: {len(df)}\")\n",
    "print(f\"   üì¶ Unique packages: {df['device_id'].nunique()}\")\n",
    "print(f\"   üìà Sensor types: {', '.join(sorted(df['data_type'].unique()))}\")\n",
    "print(f\"   üåç Cities: {df['closest_city'].nunique()}\")\n",
    "print(f\"   üìÖ Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "\n",
    "# Sensor data validation\n",
    "print(f\"\\nüîç Sensor Data Validation (ALL VALUES POSITIVE):\")\n",
    "sensor_stats = {}\n",
    "\n",
    "for sensor_type in [\"temperature\", \"humidity\", \"shock\"]:\n",
    "    sensor_data = df[df['data_type'] == sensor_type]['numeric_value']\n",
    "    if len(sensor_data) > 0:\n",
    "        stats = {\n",
    "            'count': len(sensor_data),\n",
    "            'min': sensor_data.min(),\n",
    "            'max': sensor_data.max(), \n",
    "            'mean': sensor_data.mean(),\n",
    "            'std': sensor_data.std()\n",
    "        }\n",
    "        sensor_stats[sensor_type] = stats\n",
    "        print(f\"   {sensor_type}: {stats['count']} readings\")\n",
    "        print(f\"     Range: {stats['min']:.2f} - {stats['max']:.2f}\")\n",
    "        print(f\"     Average: {stats['mean']:.2f} ¬± {stats['std']:.2f}\")\n",
    "        \n",
    "        # Validation: Ensure no negative values\n",
    "        if stats['min'] < 0:\n",
    "            print(f\"     ‚ùå ERROR: Found negative values in {sensor_type}!\")\n",
    "        else:\n",
    "            print(f\"     ‚úÖ All {sensor_type} values are positive\")\n",
    "\n",
    "# Data distribution check\n",
    "print(f\"\\nüìä Records per sensor type:\")\n",
    "for sensor in sorted(df['data_type'].unique()):\n",
    "    count = len(df[df['data_type'] == sensor])\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   {sensor}: {count} records ({percentage:.1f}%)\")\n",
    "\n",
    "# Timeline validation\n",
    "timeline_span = df['timestamp'].max() - df['timestamp'].min()\n",
    "print(f\"\\n‚è∞ Timeline validation (using ORIGINAL CSV timeline):\")\n",
    "print(f\"   Time span: {timeline_span}\")\n",
    "print(f\"   Records per day: {len(df) / max(1, timeline_span.days):.1f}\")\n",
    "print(f\"   üìÖ Data covers: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "\n",
    "# Step 6: SAVE CLEAN DATASET FOR TABLEAU/CHARTS\n",
    "print(f\"\\nüíæ Saving dataset optimized for Tableau and line charts...\")\n",
    "\n",
    "# Create the final clean dataset\n",
    "final_columns = [\n",
    "    'record_id',        # Unique identifier\n",
    "    'timestamp',        # Properly formatted datetime\n",
    "    'device_id',        # Package identifier  \n",
    "    'data_type',        # Sensor type (temperature/humidity/shock)\n",
    "    'data_value',       # Original string value\n",
    "    'numeric_value',    # Cleaned numeric value\n",
    "    'location',         # GPS coordinates\n",
    "    'closest_city',     # City name\n",
    "    'status',           # Package status\n",
    "    'origin',           # Origin city\n",
    "    'destination'       # Destination city\n",
    "]\n",
    "\n",
    "df_final = df[final_columns].copy()\n",
    "\n",
    "# Save main dataset\n",
    "try:\n",
    "    df_final.to_csv(\"cleaned_iot_data.csv\", index=False)\n",
    "    print(\"‚úÖ Main dataset saved: cleaned_iot_data.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving main dataset: {e}\")\n",
    "\n",
    "# Save sensor-specific datasets for individual analysis\n",
    "try:\n",
    "    for sensor_type in [\"temperature\", \"humidity\", \"shock\"]:\n",
    "        sensor_df = df_final[df_final['data_type'] == sensor_type].copy()\n",
    "        filename = f\"cleaned_{sensor_type}_data.csv\"\n",
    "        sensor_df.to_csv(filename, index=False)\n",
    "        print(f\"‚úÖ {sensor_type.title()} dataset saved: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving sensor datasets: {e}\")\n",
    "\n",
    "# Save summary statistics\n",
    "try:\n",
    "    summary_data = []\n",
    "    for sensor_type, stats in sensor_stats.items():\n",
    "        summary_data.append({\n",
    "            'sensor_type': sensor_type,\n",
    "            'total_records': stats['count'],\n",
    "            'min_value': stats['min'],\n",
    "            'max_value': stats['max'],\n",
    "            'avg_value': stats['mean'],\n",
    "            'std_deviation': stats['std']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(\"iot_data_summary.csv\", index=False)\n",
    "    print(\"‚úÖ Summary statistics saved: iot_data_summary.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving summary: {e}\")\n",
    "\n",
    "# Step 7: TABLEAU PREPARATION NOTES\n",
    "print(f\"\\nüìã TABLEAU PREPARATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"üìÅ FILES CREATED:\")\n",
    "print(\"   cleaned_iot_data.csv - Main dataset for Tableau dashboard\")\n",
    "print(\"   cleaned_temperature_data.csv - Temperature sensor only\")  \n",
    "print(\"   cleaned_humidity_data.csv - Humidity sensor only\")\n",
    "print(\"   cleaned_shock_data.csv - Shock sensor only\")\n",
    "print(\"   iot_data_summary.csv - Statistical summary\")\n",
    "\n",
    "print(f\"\\nüéâ SUCCESS! Dataset ready for visualization!\")\n",
    "print(f\"üìä Total: {len(df_final)} records from {df_final['device_id'].nunique()} packages\")\n",
    "print(f\"üïê Original timeline preserved: {df_final['timestamp'].min()} to {df_final['timestamp'].max()}\")\n",
    "print(f\"‚úÖ ALL VALUES POSITIVE - No negative readings in dataset\")\n",
    "print(\"üöÄ Ready for Tableau dashboard and line chart creation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
